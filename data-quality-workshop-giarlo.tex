% data-quality-workshop-giarlo.tex
% Michael J. Giarlo's position paper for the Curating for Data Quality workshop
% Author: Michael J. Giarlo
% based upon LaTeX2.09 Guidelines, 9 June 1996
% TODO: Enter date of submission here
% Revisions:	2012-08-xx

% TODO: After having produced the .bbl file (from the .bib
%     file), and prior to final submission, you need to 'insert' your .bbl
%     file into your source .tex file so as to provide one
%     'self-contained' source file.

\documentclass{acm_proc_article-sp}
\usepackage[T1]{fontenc}
% For formatting URLs in references nicely and making them clickable
% in the resulting PDF, the hyperref package is used:
\usepackage{hyperref}
\begin{document}

% TODO: Revisit this when finished
\title{Academic Libraries as Data Quality Hubs\titlenote{Paper prepared for 2012
    Curating for Data Quality workshop in Arlington, VA}}

% TODO: Revisit this when finished \subtitle{[Extended Abstract]
% \titlenote{A full version of this paper is available as
% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
% \LaTeX$2_\epsilon$\ and BibTeX} at
% \texttt{www.acm.org/eaddress.htm}}}

\numberofauthors{1} \author{
  \alignauthor Michael J. Giarlo\\
  \affaddr{Penn State University}\\
  \affaddr{E-017 Paterno Library}\\
  \affaddr{University Park, PA  16802}\\
  \email{michael@psu.edu} }
\maketitle

% TODO: Revisit this when finished
\begin{abstract}
  This is a position paper about ensuring data quality for e-science via
  curatorial practices.
  Take some language out of the first few sections, tie back to the
  paper title.
\end{abstract}

% TODO: Revisit this when finished
% http://www.acm.org/about/class/how-to-use
% http://www.acm.org/about/class/1998
% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
% A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}

% TODO: Revisit this when finished
\terms{Theory}

% TODO: Revisit this when finished
\keywords{data curation, digital preservation, academic libraries,
  stewardship, e-science, research data, trust}

% TODO: Replace all double-dashes with triple-dashes to ensure they
%       are picked up as m-dashes
% TODO: Run flyspell-buffer and make spelling corrections
\section{Scientific Data at Risk}
Data quality is a pressing, not to mention costly, issue in industry;
a 2002 study\cite{russom:case} calculated that over \$600 billion per
year was spent on ``data quality problems''\cite{eckerson:bottomline}.
At the same time, data quality issues have become an area of growing
attention within academia and academic libraries
\cite{heidorn:libraries,arl:stewardship,ogburn:imperative,jisc:deluge},
as scientific practices evolve to take advantage of robust campus
cyberinfrastructure and as funding agencies, such as the National
Science Foundation and the National Institutes of Health, increasingly
require data management plans to protect and amplify the impact of
their investments.

As computing costs have dwindled, processing speed, network speed, and
storage capacity have grown, resulting in an explosion of scientific
data.  Experiments, in some disciplines more than others, are
producing more data than their principal investigators and research
assistants can handle \cite{adams:galaxyzoo}. Due to the wealth of
data that is being produced, scientific practice is changing; the
gathering of data for one experiment may serve dozens or hundreds of
other experiments around the world \cite{jisc:deluge}.

Data is more abundant than ever before, and no less important, and yet
it is at risk \cite{ogburn:imperative,heidorn:libraries}.  ``The
survival of this data is in question since the data are not housed in
long-lived institutions such as libraries. This situation threatens
the underlying principles of scientific replicability since in many
cases data cannot readily be collected again''
\cite{heidorn:libraries}. There are numerous examples in the
literature of analog data enabling scientific inquiry decades and
longer past the date it was gathered\footnote{Ogburn
  \cite{ogburn:imperative} cites Stephen Jay Gould's ``The Mismeasure
  of Man'' in which we learn that ``analysis and critique of cranial
  measurements in the 1800s, twin studies in the 1950s, and the rise
  of IQ testing were possible because the data were still available
  for scrutiny and replication''} How do we as a society, and
particularly we within academia, not only preserve this wealth of data
for future science but ensure its quality?

\subsection{Curatorial Practice and Challenges}

Cultural heritage organizations such as libraries and archives have
been stewards of society's cultural and scientific assets for
millennia, providing public access to high-quality collections, 0and
they remain so in the Internet age. Though the activities involved are
different for analog assets, ``[s]tewardship of digital resources
involves both preservation and curation. Preservation entails
standards-based, active management practices that guide data
throughout the research life cycle, as well as ensure the long-term
usability of these digital resources. Curation involves ways of
organizing, displaying, and repurposing preserved data''
\cite{arl:stewardship}.

Digital preservation and digital curation, though relatively new
practices, are widely cited in the literature
\cite{jisc:deluge,curry:community,goble:curation,ogburn:imperative,heidorn:libraries,williams:lifecycle,arl:stewardship}. Digital
curation aims to make selected data accessible, usable, and useful
throughout its lifecycle. Digital curation subsumes digital
preservation; without viable data, which digital preservation enables,
there's nothing to be curated\footnote{This characterization of
  digital curation and digital preservation is a mere gloss; more may
  be found, for instance, on the Digital Curation Centre's website:
  \url{http://www.dcc.ac.uk/digital-curation}.}.

An oft-cited mantra around the practice of digital curation is that
``curation begins before creation [of the data]''
\cite{rusbridge:curation}. And yet, ``[b]y the time knowledge in
digital form makes its way to a safe and sustainable repository [such
as those provided by academic libraries], it may be unreadable,
corrupted, erased, or otherwise impossible to recover and
use. Scientific data files may be especially endangered due to their
sheer size, computational elements, reliance on and integration with
software, associated visualizations, few or competing standards,
distributed ownership, dispersed storage, inaccessibility, lack of
documented provenance, complex and dynamic nature, and the concomitant
need for a specialized knowledge base -- and experience -- to handle
data.  Data also may be endangered by the practices of scholars who
regard their data as having little value beyond the confines of a
small group, a specific project, or a specified period''
\cite{ogburn:imperative}.

Since digital curation is a new practice, and is generally centered
within cultural heritage organizations (rather than within the
research enterprise), \textit{post-hoc} curation is an unfortunate
fact of life; researchers lack the incentive, the resources, the time,
or the expertise to curate their own data\footnote{Hereafter referred
  to as ``sheer curation or curation at source''
  \cite{curry:community}.}. For some massive data sets, furthermore,
it is difficult to imagine, \textit{e.g.}, a research institute or
science department ever having the resources to curate their data at
the source.

The practice of \textit{post-hoc} curation (vs. ``sheer curation'') is
less than ideal for a number of reasons.

First, one of the goals of curation is to enable the usefulness of a
digital resource over time, and one of the tactics in this area is to
provide sufficient context for a resource such that future users can
understand what an object is, where it came from, why it is
significant, and how to use it. Context is often provided via
documentation, descriptive metadata, or
both\cite{arl:stewardship,heidorn:libraries,curry:community,jisc:deluge}. The
creator(s) of the data, not its \textit{post-hoc} curators, are best
equipped to provide this context; to get a sense of this distinction,
consider the difference between cataloging your own book collection
and cataloging a complete stranger's book collection.

The second reason, building on the prior reason, is that
\textit{post-hoc} curation happens some time, possibly a sufficiently
long enough time to lose sight of important information, after the
data have been created; capturing the context around a data set is
best done while the data is still fresh in its creator's mind,
\textit{i.e.}, before or during its creation. Documentation or
metadata that is created by a party other than the data's creator will
suffer from this lack of context.

``This [\textit{post-hoc} curation] activity is to provide
representational information and description. This is particularly
problematic for academic libraries, since the data being generated at
research and teaching institutions are incredibly varied. Many
representational schemes for the data and metadata will be
required. No one individual will have all of the required skills. Data
curators will need to collaborate closely with the data providers to
understand the data''\cite{heidorn:libraries}. Whether the researchers
will have sufficnet time, resources, and inclination to collaborate
with academic libraries on the work of curating research data at scale
is yet to be seen.

The final reason, and possibly the most limiting, is that of the
misalignment between the scale of the need for on-campus data curation
and the level of commitment by academic libraries to address this
need (as measured by the amount of resources allocated to this need
vs. other needs).

Academic libraries are nonetheless uniquely positioned to address the
problem of data quality in e-science by virtue of their record of
effective stewardship, their commitment to providing access to
high-quality data over the long-term, and their expertise in digital
preservation and digital curation practices, as ``[digital] curation is a
process that can ensure the quality of data and its fitness for
use''\cite{curry:community}.  It is worth examining this claim in the
context of a framework that measures data quality.

% TODO: Revisit whether this section is needed. Not coming together at
%       the moment.
%\subsection{Trust Networks, Briefly Considered}
%
%While cyberinfrastructure has become more robust, usage of the
%Internet continued to grow; between 2000 and 2010 alone, Internet
%usage rose from nearly 7\% of the global population to over 30\%
%\footnote{Data visualized here:\url{http://www.google.com/publicdata/explore?ds=d5bncppjof8f9_&met_y=it_net_user_p2&tdim=true&dl=en&hl=en&q=global+internet+usage}}.
%trust networks - how are they different now? japanese research example
%(more numerous and more anonymous) \cite{timmer:faking}, galaxyzoo
%example (much larger and more participatory (citizen
%science)) \cite{adams:galaxyzoo}.

\section{Measuring Data Quality}
There are a number of theoretical frameworks examining data quality
measures already available, and Knight's 2005 paper compares a
selection of a dozen ``widely accepted [information quality]
Frameworks collated from the last decade of [information science]
research'' \cite{knight:quality}. Common features are identified for
data quality (or information quality), such as that it is a concept
with multiple dimensions, wherein the overall quality is a function of
successive indicators.  Another common feature of data quality
frameworks is the grouping of quality indicators into categories,
classes, or levels corresponding to, \textit{e.g.}, semiotic levels,
layers of intrinsicity and extrinsicity, and the subjectivity /
objectivity spectrum.

The following framework is distilled from Knight's comparison of
quality frameworks, and constitutes ``a series of quality dimensions
which represent a set of desirable characteristics for an information
resource'' \cite{curry:community}. The framework is then applied to
the domain of research data quality as viewed from my perspective,
that of a digital preservation technologist and practitioner of
digital curation. It is not offered as a novel framework, nor a
comprehensive one, but merely as a tool for understanding and
evaluating the applicability of digital curation and preservation
practices to the measure of data quality.

\begin{description}
\item[Trust] \hfill \\
  Evaluation of the extent to which data is trusted depends on a set
  of subjective factors, including whether the data is judged to be
  authentic, the reputation of the party/ies responsible for the data,
  and the biases of the person who is evaluating the
  data\footnote{Trust is a complex issue that though relevant is too
    far-reaching to be within the the scope of this position paper. It
    is nonetheless listed in the framework, at the very top of the
    framework, to establish that the lower layers may be entirely
    discounted by an individual judging data quality if there are
    overriding trust issues.}.
\item[Authenticity] \hfill \\
  Evaluation of the authenticity of data requires that data be
  understood. Authenticity in this context is a rough measure of the
  extent to which the data is judged to be ``good science,'' answering
  questions pertaining to, \textit{e.g.}, the reliability of the
  instruments used to gather the data; the soundness of underlying
  theoretical frameworks; the completeness, accuracy, and validity of
  the data; and ontological consistency within the data.
\item[Understandability] \hfill \\
  Evaluation of the understandability of data requires that there be
  sufficient context (documentation, metadata, and provenance)
  describing the data, and that the data is usable.
\item[Usability] \hfill \\
  Usability of data requires that data are discoverable and
  accessible; that data are in a usable file format; that the
  individual judging the data's quality has an appropriate tool to
  access the data; and that the data are fixed.
\item[Integrity] \hfill \\
  Integrity of data assumes that the data can be proven to be
  identical, at the bit level, to some prior accepted or verified
  state. Data integrity is required for usability, understandability,
  authenticity, trust, and thus overall quality.
\end{description}

The relationship between the quality dimensions in this framework is
analogous to that of the Semantic Web Layer Cake in that ``each layer
exploits and uses capabilities of the layers below''
\cite{wiki:semweb}. Viewed from the bottom up, this framework asserts
that data integrity is necessary but not sufficient for data quality;
if the data lacks integrity, it may not be usable, and thus not
understandable, authenticatable, or trustable -- a very low measure of
quality. Viewed from the top down, on the other hand, if an individual
trusts a data set, she likely judges it to be of the highest
quality.

\section{Applying Curation to Data Quality}

Within the defined framework, how might the practice of curation
help ensure data quality?

The curation lifecycle \cite{dcc:lifecycle} contains actions geared
towards preservation of the digital asset, which includes
bit-preservation via a number of possible tactics such as regular
digital signature (or checksum) verification, replication, media
refreshing, version management, and file-level backups. These tactics
taken together should be sufficient to ensure that the data remains in
the same state as originally processed. Assuming that the data was
authentic to begin with\footnote{Authenticity is evaluated higher up
  the stack.}, the effective practice of curation should provide data
integrity.

Three of the seven sequential actions defined in the lifecycle model
have a direct impact on the usability of data. First, the Create or
Receive action\footnote{Again underscoring the mantra that ``curation
  begins before creation''} should include determination of an
appropriate file format for the data, choosing a format that is judged
to be widely accessible and preservable. The Access, Use, \& Reuse
action ``[e]nsure[s] that data is accessible to both designated users
and reusers, on a day-to-day basis'', thus ensuring that the data are
discoverable and made available to potential users of data. The
Transform action, lastly, includes periodic evaluation of file formats
and migration to new formats so data remain usable well after the
original formats have been rendered obsolete.

Context is provided for data, so that users may understand the data,
both in sequential actions within the curation lifecycle --- those
being Create or Receive and Preservation Action --- and also within
the full lifecycle action of Description and Representation
Information. The generation, extraction, and application of metadata
by machine agents and humans is thus a key part of the curation
lifecycle, providing periodic management and addition of context to
data.  These actions make sure the data's purpose, its impact, and its
provenance are established over the course of its lifecycle so that
current and future users can make sense of data that they have
discovered.

Authenticity and trust as dimensions of data quality are highly
subjective.  The curation process can document what instruments are
used to generate data, but not how reliable a user judges those
instruments to be; it can include metadata about the theoretical
frameworks underlying the data, but not whether the frameworks are
theoretically sound; it can clearly establish the parameters of the
data, but it is up to the user to judge whether those are a complete
or incomplete set of parameters. The context, provenance, and
documentation provided by curation are thus critically important in
arming users of data with the metadata they need to make quality
judgments but are not capable of independently ensuring data
authenticity or trust in data.

\section{Areas of Opportunity}

\subsection{Curation Models}
Back to the issue of sheer curation vs. post-hoc curation.

The work involved with sheer curation needs to be incentivized,
however, and/or integrated into the researcher's extant workflow else
we've learned that curation will be an after-thought. post-hoc
curation also contributes to the problem of scaling
up\cite{curry:community} research data curation efforts in academia
(establish this as a problem per se earlier and tie in the
crowd-sourcing and automated tools angles?) it's also worth noting
that sheer curation and post-hoc curation are not mutually exclusive.

\subsubsection{Alternative Models}
Automatic curation and crowd-sourcing should be considered as tiers more
broadly to deal with issues around scale

on the idea of levels, tiers, or types of curation: ``Data curation
teams have found it difficult to scale the traditional centralized
approach and have tapped into community crowd-sourcing and automated
and semi-automated curation algorithms.''\cite{curry:community}

on crowd-sourcing as an alternative to sheer curation and proxy
curation, specifically as a tactic that can be applied as a stopgap
proxy measure. learn from galaxyzoo/zooniverse example: interesting
data, compellingly visualized; incentives like competition and game
feel and just plain involvement with real research with an opportunity
to make a novel scientific discovery (``Hanny's Voorwerp''); balance
crowd curation with expert verification \cite{adams:galaxyzoo};
reliance on network effects to achieve accurate data over time;
support formation of user-created and -managed communities (interest
groups and moderators and subforums \cite{adams:galaxyzoo}; identify,
engage, and nurture 'champions' or deputies in the community; expand
the model to other domains if successful (zooniverse); and implicit in
the Zooniverse examples is the principle well known in the open source
software world that products that are defective in some way are more
likely to be discovered with more eyes on them (quality by consensus)

crowd-sourcing successes (+) and challenges (-). galaxyzoo is a great
success story (+/ 250K users classified 100s of millions
\cite{adams:galaxyzoo}). community building (-), incentivizing (-),
critical mass for network effects (-), wikipedia (+), NYT model (+/
sheer and post-hoc ``trust but verify'' model), ChemSpider (+),
Reuters/Calais (+), PDB (+).  social best
practices\cite{curry:community}: early and sustained stakeholder
involvement; outreach beyond the community via multiple channels;
connection of curation activities to a tangible payoff; an appropriate
and clear governance model. technical best practices: more or less
community standard data representations; balance between automated and
human curation with the latter always beating the former; record and
make available provenance events (trust)

map the social and technical best practices from elsewhere to the
framework.

need better tools (enumerate what makes 'em better) for curation/data
integrity (does ScholarSphere help?) tools for automated curation
(such as subject classification, POS tagging, entity extraction,
automatic vocabulary assignment, and characterization) help with the
scalability somewhat, though many of these tools are optimized for
text-type content, not for e.g. numeric or statistical data though
some of these processes are easier than others (deduplication,
normalization of forms, etc.)

\subsection{Roles on Campus}

The activities required for curation and for preservation also require
collaboration between libraries and researchers, if libraries are to
play a role in in providing data quality on campus. The DMP
requirement gives libraries an 'in.' libraries' experience can be
particularly helpful during the data collection phase (metadata, file
format standards, provenance, etc.) and also during appraisal and
selection, as librarians have been doing collection development for
centuries. there is also a role at this stage for collaborative
modeling: determining boundaries between e.g. objects and bitstreams,
deciding which things will receive new persistent identifiers, etc.

some \cite{heidorn:libraries} argue that the library is the natural
place for digital data to then be deposited, preserved, and
migrated. while I agree that from a consolidation of services
perspective, it makes sense to have these functions centrally provided
on campus, the library may or may not be the best service provider --
central IT, for instance, may have more robust infrastructure around
information security, identity and access management, disaster
recovery, high availability, hardware lifecycling, media refresh, and
so forth. a partnership between the libraries, which have experience
and expertise in curatorial and preservation practices, and a central
repository service provider (whether that's the library or not) seems
most conducive to the long-term viability of research data.

where libraries lack the resources to serve as a central service
provider, they may play a less active and equally vital role. use
subject liaison connections for outreach and marketing, plus
connections to campus research/sponsored programs office, and offer
services around information clearinghouse (``if you're in Bio, you can
submit to these subject repos, and if you're in Linguistics, you can
take advantage of this other central IT service, etc.''). libraries
can also help w/ instruction particularly around practical tools and
processes pertaining to personal digital curation
\cite{williams:lifecycle}, especially helpful in organizations where
the culture is that of extreme decentralization or sparse
collaboration.

\subsection{Education, Outreach, Training, and Acculturation}

value of curation, learning that early, using it early in research process -
tie in to crowdsourcing adding value (galaxyzoo) - hack academic culture in
such a way that good curatorial skills are necessary for good science, need
``data science'' programs that marry scientific methodology with data curation
and retention practices \cite{uw:datascience}

the onus is on cultural heritage institutions to make the value-added
argument regarding curation and preservation of data to researchers.
funding agencies can help with this, as can academic research offices
and subject disciplines and institutes, where such strategic
partnerships are viable.  also: IT has a lot to offer in this domain,
opportunity for increased partnership.

``There is a need for a close linking between digital data archives,
scholarly publications, and associated communication. The potential
for an expanded role for research libraries in the area of digital
data stewardship affords opportunities to address these important
linkages.'' \cite{arl:stewardship}

``A change in both the culture of federal funding agencies
and of the research enterprise regarding digital data stewardship is
necessary if the programs and initiatives that support the long-term
preservation, curation, and stewardship of digital data are to be
successful.'' \cite{arl:stewardship}

this may require reskilling, reeducating, rehiring, reorganizing,
retraining, embedding, and more. substantiated in \cite{jisc:deluge}
and also there's this: ``Knowing that we should not and cannot save
everything, librarians should apply archival strategies, principles,
and practices to selection and curation of data. We may start by
identifying at-risk data that require urgent attention. A survey of
our special and general collections may discover and expose data we
already own amid our other holdings. Our current methods of cataloging
and metadata application will likely need enhancement''
\cite{ogburn:imperative} (archival principles!)

librarians need the sort of skills to pull this off, etc. (riff on
this some more, about librarians needing to evolve to manage the
scalability issues around doing campuswide preservation and curation
of reserach data.) so too must researchers!  incorporate data mgmt
into info lit courses, or add new data lit courses for undergrads and
grads, encourage attendance as part of DMPs?

``funding and planning for the care and retention of data must be
built into the front end, not the back end, of the research
process. Data files must be attended to while they are compiled and
analyzed in order to keep them available for a reasonable life
span. This will require librarians to be conversant with the language
and methods of science, at the table for campus cyberinfrastructure
planning, and working with researchers at the beginning stages of
grant planning'' \cite{ogburn:imperative}

data science programs such as \cite{uw:datascience} are a good start,
but could use more of an emphasis on data quality and curation in
addition to gathering, analysis, and visualization.

refer to \cite{timmer:faking} issue. hacking trust networks, new trust
networks, more resilient trust networks -- look into identity
assurance (concept from federated identity and access management area)
and assurance levels, specifically applying it to data authenticity
(now that researcher ID systems such as ORCID or ISNI are gaining
steam, perhaps requiring more than signatures (OAuth via ORCID, etc.)
could help, as could karma-like systems) ``assurance levels'' as relevant
concepts?

% TODO: Revisit this when finished The following two commands are all
% you need in the initial runs of your .tex file to produce the
% bibliography for the citations in your paper.
\bibliography{data-quality-workshop-giarlo}{}
\bibliographystyle{abbrv}

% TODO: Revisit this when finished You must have a proper ".bib" file
% and remember to run: latex bibtex latex latex to resolve all
% references ACM needs 'a single self-contained file'!

% TODO: Revisit this when finished
% http://www.acm.org/sigs/publications/sigguide-v2.2sp (sec 2.8)
% \balancecolumns

\end{document}
