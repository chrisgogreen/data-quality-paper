% data-quality-workshop-giarlo.tex
% Michael J. Giarlo's position paper for the Curating for Data Quality workshop
% Author: Michael J. Giarlo
% based upon LaTeX2.09 Guidelines, 9 June 1996
% TODO: Enter date of submission here
% Revisions:	2012-08-xx

% TODO: After having produced the .bbl file (from the .bib
%     file), and prior to final submission, you need to 'insert' your .bbl
%     file into your source .tex file so as to provide one
%     'self-contained' source file.

\documentclass{acm_proc_article-sp}
\begin{document}

% TODO: Revisit this when finished
\title{The Data Quality Layer Cake\titlenote{Paper prepared for 2012
    Curating for Data Quality workshop in Arlington, VA}}

% TODO: Revisit this when finished \subtitle{[Extended Abstract]
% \titlenote{A full version of this paper is available as
% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
% \LaTeX$2_\epsilon$\ and BibTeX} at
% \texttt{www.acm.org/eaddress.htm}}}

\numberofauthors{1} \author{
  \alignauthor Michael J. Giarlo\\
  \affaddr{Penn State University}\\
  \affaddr{E-017 Paterno Library}\\
  \affaddr{University Park, PA  16802}\\
  \email{michael@psu.edu} }
\maketitle

% TODO: Revisit this when finished
\begin{abstract}
  This is a position paper about ensuring data quality for e-science via
  curatorial practices.
\end{abstract}

% TODO: Revisit this when finished
% http://www.acm.org/about/class/how-to-use
% http://www.acm.org/about/class/1998 A category with the (minimum)
% three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
% A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}

% TODO: Revisit this when finished
\terms{Theory}

% TODO: Revisit this when finished
\keywords{data curation, digital preservation, trust}

\section{Gap Analysis / Challenges}
More about what problems we're elucidating and for which we're proposing
solutions.

From wikipedia article on Data Quality: ``One industry study estimated
the total cost to the US economy of data quality problems at over
US\$600 billion per annum (Eckerson, 2002)''\cite{eckerson:bottomline}

Science is changing -- how can advances in curation practices aid the
practice of good science?

post-hoc curation is a fact of life, as researchers lack the
incentive, the resources, the time, or the expertise to curate their
own data. this is troubling for the context/documentation/metadata of
the data, as the creators of the data are best equipped to provide
this context -- and for the sake of timeliness, while the data are
fresh in the researcher's mind (``sheer curation or curation at
source''\cite{curry:community}), it is ideal for this to take place at
the time of the data's creation.  data that is curated by a party
other than its creator may suffer from this lack of context.  this
work needs to be incentivized, however, and/or integrated into the
researcher's extant workflow else we've learned that curation will be
an after-thought. post-hoc curation also contributes to the problem of
scaling up\cite{curry:community} research data curation efforts in
academia (establish this as a problem per se earlier and tie in the
crowd-sourcing and automated tools angles?) it's also worth noting
that sheer curation and post-hoc curation are not mutually exclusive.

crowd-sourcing successes (+) and challenges (-). galaxyzoo is a great
success story (+). community building (-), incentivizing (-), critical
mass for network effects (-), wikipedia (+), NYT model (+/ sheer and
post-hoc ``trust but verify'' model), ChemSpider (+), Reuters/Calais
(+), PDB (+), 

tools for automated curation (such as subject classification, POS
tagging, entity extraction, and characterization) help with the
scalability somewhat, though many of these tools are optimized for
text-type content, not for e.g. numeric or statistical data though
some of these processes are easier than others (deduplication,
normalization of forms, etc.)

What is data quality?

How is data quality ensured?

trust networks - how are they different now? japanese research example (more
anonymous), galaxyzoo example (much larger). ``quality indicators'' and
``assurance levels''

\section{The Framework}
A framework for understanding data quality is as follows.  data
quality stack, quality as a function of successive indicators. (less
subjective -> more subjective, up the layers)

not meant to be comprehensive, but rather ``a series of quality
dimensions which represent a set of desirable characteristics for an
information resource''\cite{curry:community} including the following:
discoverability/accessibility (curation helps), completeness (curation
helps), interpretation (context helps, can be provided by curation),
accuracy, consistency, provenance/reputation (documentation/context,
authenticity, and trust networks), timeliness.



\begin{itemize}
\item Highest quality data is authentic (``good science:'' reliable
  instruments, sound theoretical frameworks, completeness, validity,
  ontological consistency, accuracy)
\item Evaluation of the authenticity of data requires that data be usable
  (curation; domain-specific tools, repos, and conventions; file formats)
\item Usability of data requires that data be accessible, discoverable
  (metadata), identifiable (citation, management (niche for cultural
  heritage orgs)), and associated with sufficient
  documentation/context to use it (curation processes)
\item Data integrity is required for context, usability, authenticity, and
  quality.
\end{itemize}

underlying assumption: data preservation (integrity) is necessary but not
sufficient for data quality. also: context (curation), usability (identity,
discoverability), trust (authenticity). adds up to quality. related to
``Semantic Web Stack''

\section{Angles of Attack}
Recommendations on ways to resolve the problems above.

need better tools (enumerate what makes 'em better) for curation/data
integrity (does ScholarSphere help?)

value of curation, learning that early, using it early in research process -
tie in to crowdsourcing adding value (galaxyzoo) - hack academic culture in
such a way that good curatorial skills are necessary for good science, need
``data science'' programs that marry scientific methodology with data curation
and retention practices

hacking trust networks, new trust networks, more resilient trust networks

% TODO: Revisit this when finished The following two commands are all
% you need in the initial runs of your .tex file to produce the
% bibliography for the citations in your paper.
\bibliography{data-quality-workshop-giarlo}{}
\bibliographystyle{abbrv}

% TODO: Revisit this when finished You must have a proper ".bib" file
% and remember to run: latex bibtex latex latex to resolve all
% references ACM needs 'a single self-contained file'!

% TODO: Revisit this when finished
% http://www.acm.org/sigs/publications/sigguide-v2.2sp (sec 2.8)
% \balancecolumns

\end{document}
