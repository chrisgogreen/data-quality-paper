% data-quality-workshop-giarlo.tex
% Michael J. Giarlo's position paper for the Curating for Data Quality workshop
% Author: Michael J. Giarlo
% based upon LaTeX2.09 Guidelines, 9 June 1996
% TODO: Enter date of submission here
% Revisions:	2012-08-xx

% TODO: After having produced the .bbl file (from the .bib
%     file), and prior to final submission, you need to 'insert' your .bbl
%     file into your source .tex file so as to provide one
%     'self-contained' source file.

\documentclass{acm_proc_article-sp}

% For formatting URLs in references nicely and making them clickable
% in the resulting PDF, the hyperref package is used:
\usepackage{hyperref}
\begin{document}

% TODO: Revisit this when finished
\title{The Data Quality Layer Cake\titlenote{Paper prepared for 2012
    Curating for Data Quality workshop in Arlington, VA}}

% TODO: Revisit this when finished \subtitle{[Extended Abstract]
% \titlenote{A full version of this paper is available as
% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
% \LaTeX$2_\epsilon$\ and BibTeX} at
% \texttt{www.acm.org/eaddress.htm}}}

\numberofauthors{1} \author{
  \alignauthor Michael J. Giarlo\\
  \affaddr{Penn State University}\\
  \affaddr{E-017 Paterno Library}\\
  \affaddr{University Park, PA  16802}\\
  \email{michael@psu.edu} }
\maketitle

% TODO: Revisit this when finished
\begin{abstract}
  This is a position paper about ensuring data quality for e-science via
  curatorial practices.
\end{abstract}

% TODO: Revisit this when finished
% http://www.acm.org/about/class/how-to-use
% http://www.acm.org/about/class/1998 A category with the (minimum)
% three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
% A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}

% TODO: Revisit this when finished
\terms{Theory}

% TODO: Revisit this when finished
\keywords{data curation, digital preservation, trust}

\section{Gap Analysis / Challenges}
More about what problems we're elucidating and for which we're proposing
solutions.

From wikipedia article on Data Quality: ``One industry study estimated
the total cost to the US economy of data quality problems at over
US\$600 billion per annum (Eckerson, 2002)''\cite{eckerson:bottomline}

Science is changing -- how can advances in curation practices aid the
practice of good science?

Basic e-science value proposition for libraries: ``Scientists,
other scholars, and all of society are now producing, storing, and
disseminating digital data that underpin the aforementioned documents
in much larger volumes than the text. The survival of this data is in
question since the data are not housed in long-lived institutions such
as libraries. This situation threatens the underlying principles of
scientiﬁc replicability since in many cases data cannot readily be
collected again. Libraries are the institutions that could best manage
this intellectual output.'' \cite{heidorn:libraries}

This should be possible in e-science: ``In The Mismeasure of Man,
geologist, biologist, and historian of science Stephen Jay Gould
exposed the biases of research on race to illuminate the resulting
scientific, historical, sociological, and political ramifications.9
His analysis and critique of cranial [End Page 242] measurements in
the 1800s, twin studies in the 1950s, and the rise of IQ testing were
possible because the data were still available for scrutiny and
replication.'' \cite{ogburn:imperative}

Data is more important and more abundant than ever, and more at risk
\cite{ogburn:imperative} than ever. curation needs to take place ASAP,
and libraries have a role to play.  long-term care of digital data is
``up for grabs'', and libraries have a track record of stewardship:
``By the time knowledge in digital form makes its way to a safe and
sustainable repository, it may be unreadable, corrupted, erased, or
otherwise impossible to recover and use. Scientific data files may be
especially endangered due to their sheer size, computational elements,
reliance on and integration with software, associated visualizations,
few or competing standards, distributed ownership, dispersed storage,
inaccessibility, lack of documented provenance, complex and dynamic
nature, and the concomitant need for a specialized knowledge base—and
experience—to handle data.  Data also may be endangered by the
practices of scholars who regard their data as having little value
beyond the confines of a small group, a specific project, or a
specified period.'' \cite{ogburn:imperative}

establish stewardship as a set of services including (but not limited
to) curation and preservation: ``Stewardship of digital resources
involves both preservation and curation. Preservation entails
standards-based, active management practices that guide data
throughout the research life cycle, as well as ensure the long-term
usability of these digital resources. Curation involves ways of
organizing, displaying, and repurposing preserved data.''
\cite{arl:stewardship}

post-hoc curation is a fact of life, as researchers lack the
incentive, the resources, the time, or the expertise to curate their
own data. this is troubling for the context / documentation / metadata
of the data, as the creators of the data are best equipped to provide
this context -- and for the sake of timeliness, while the data are
fresh in the researcher's mind (``sheer curation or curation at
source''\cite{curry:community}), it is ideal for this to take place at
the time of the data's creation.  data that is curated by a party
other than its creator may suffer from this lack of context.  this
work needs to be incentivized, however, and/or integrated into the
researcher's extant workflow else we've learned that curation will be
an after-thought. post-hoc curation also contributes to the problem of
scaling up\cite{curry:community} research data curation efforts in
academia (establish this as a problem per se earlier and tie in the
crowd-sourcing and automated tools angles?) it's also worth noting
that sheer curation and post-hoc curation are not mutually exclusive.

difficulty of sheer curation: ``This activity is to provide
representational information and description. This is particularly
problematic for academic libraries, since the data being generated at
research and teaching institutions are incredibly varied. Many
representational schemes for the data and metadata will be
required. No one individual will have all of the required skills. Data
curators will need to collaborate closely with the data providers to
understand the data.'' \cite{heidorn:libraries}

The activities required for curation and for preservation also require
collaboration between libraries and researchers, if libraries are to
play a role in in providing data quality on campus. The DMP
requirement gives libraries an 'in.' libraries' experience can be
particularly helpful during the data collection phase (metadata, file
format standards, provenance, etc.) and also during appraisal and
selection, as librarians have been doing collection development for
centuries.

crowd-sourcing successes (+) and challenges (-). galaxyzoo is a great
success story (+). community building (-), incentivizing (-), critical
mass for network effects (-), wikipedia (+), NYT model (+/ sheer and
post-hoc ``trust but verify'' model), ChemSpider (+), Reuters/Calais
(+), PDB (+).  social best practices\cite{curry:community}: early and
sustained stakeholder involvement; outreach beyond the community via
multiple channels; connection of curation activities to a tangible
payoff; an appropriate and clear governance model. technical best
practices: more or less community standard data representations;
balance between automated and human curation with the latter always
beating the former; record and make available provenance events
(trust)

Good quote: ``With increased utilization of data within their
operational and strategic processes, enterprises need to ensure data
quality and accuracy. Data curation is a process that can ensure the
quality of data and its ﬁtness for use. Data curation teams have found
it difﬁcult to scale the traditional centralized approach and have
tapped into com- munity crowd-sourcing and automated and
semi-automated curation algorithms.''\cite{curry:community} as it
establishes link between quality and curation, and cites scale as a
blocking issue w/ traditional curation model.

tools for automated curation (such as subject classification, POS
tagging, entity extraction, automatic vocabulary assignment, and
characterization) help with the scalability somewhat, though many of
these tools are optimized for text-type content, not for e.g. numeric
or statistical data though some of these processes are easier than
others (deduplication, normalization of forms, etc.)

What is data quality?

How is data quality ensured?

trust networks - how are they different now? japanese research example (more
anonymous), galaxyzoo example (much larger). ``quality indicators'' and
``assurance levels''

\section{The Framework}
A framework for understanding data quality is as follows.  data
quality stack, quality as a function of successive indicators
(commonly cited in the literature as ``a multi-dimensional concept''
\cite{knight:quality}). (less subjective -> more subjective, up the
layers)

not meant to be comprehensive -- as there are plenty of those models
including the dozen cited in this 2005 paper \cite{knight:quality},
though in fact this framework maps well to many of the these
better-developed models -- but rather ``a series of quality dimensions
which represent a set of desirable characteristics for an information
resource'' \cite{curry:community} including the following:
discoverability/accessibility (curation helps), completeness (curation
helps), interpretation (context helps, can be provided by curation),
accuracy, consistency, provenance/reputation (documentation/context,
authenticity, and trust networks), timeliness.

map the social and technical best practices from the prior section to
the framework

\begin{itemize}
\item Highest quality data is authentic (``good science:'' reliable
  instruments, sound theoretical frameworks, completeness, validity,
  ontological consistency, accuracy)
\item Evaluation of the authenticity of data requires that data be usable
  (curation; domain-specific tools, repos, and conventions; file formats)
\item Usability of data requires that data be accessible, discoverable
  (metadata), identifiable (citation, management (niche for cultural
  heritage orgs)), and associated with sufficient documentation /
  context / provenance to use it (curation processes) (in support of
  context: ``More sophisticated automation will be required for the
  management of data and its storage. Automated knowledge management
  will be needed to explore and exploit data, at several
  levels. Metadata is key to this capability.'' \cite{jisc:deluge})
\item Data integrity is required for context, usability, authenticity, and
  quality.
\end{itemize}

underlying assumption: data preservation (integrity) is necessary but not
sufficient for data quality. also: context (curation), usability (identity,
discoverability), trust (authenticity). adds up to quality. related to
``Semantic Web Stack''

\section{Angles of Attack}
Recommendations on ways to resolve the problems above.

need better tools (enumerate what makes 'em better) for curation/data
integrity (does ScholarSphere help?)

value of curation, learning that early, using it early in research process -
tie in to crowdsourcing adding value (galaxyzoo) - hack academic culture in
such a way that good curatorial skills are necessary for good science, need
``data science'' programs that marry scientific methodology with data curation
and retention practices

the onus is on cultural heritage institutions to make the value-added
argument regarding curation and preservation of data to researchers.
funding agencies can help with this, as can academic research offices
and subject disciplines and institutes, where such strategic
partnerships are viable.  also: IT has a lot to offer in this domain,
opportunity for increased partnership.

``There is a need for a close linking between digital data archives,
scholarly publications, and associated communication. The potential
for an expanded role for research libraries in the area of digital
data stewardship affords opportunities to address these important
linkages.'' \cite{arl:stewardship}

``A change in both the culture of federal funding agencies
and of the research enterprise regarding digital data stewardship is
necessary if the programs and initiatives that support the long-term
preservation, curation, and stewardship of digital data are to be
successful.'' \cite{arl:stewardship}

this may require reskilling, reeducating, rehiring, reorganizing,
retraining, embedding, and more. substantiated in \cite{jisc:deluge}
and also there's this: ``Knowing that we should not and cannot save
everything, librarians should apply archival strategies, principles,
and practices to selection and curation of data. We may start by
identifying at-risk data that require urgent attention. A survey of
our special and general collections may discover and expose data we
already own amid our other holdings. Our current methods of cataloging
and metadata application will likely need enhancement''
\cite{ogburn:imperative} (archival principles!)

librarians need the sort of skills to pull this off, etc. (riff on
this some more, about librarians needing to evolve to manage the
scalability issues around doing campuswide preservation and curation
of reserach data.) so too must researchers!  incorporate data mgmt
into info lit courses, or add new data lit courses for undergrads and
grads, encourage attendance as part of DMPs?

``funding and planning for the care and retention of data must be
built into the front end, not the back end, of the research
process. Data files must be attended to while they are compiled and
analyzed in order to keep them available for a reasonable life
span. This will require librarians to be conversant with the language
and methods of science, at the table for campus cyberinfrastructure
planning, and working with researchers at the beginning stages of
grant planning'' \cite{ogburn:imperative}

hacking trust networks, new trust networks, more resilient trust networks

% TODO: Revisit this when finished The following two commands are all
% you need in the initial runs of your .tex file to produce the
% bibliography for the citations in your paper.
\bibliography{data-quality-workshop-giarlo}{}
\bibliographystyle{abbrv}

% TODO: Revisit this when finished You must have a proper ".bib" file
% and remember to run: latex bibtex latex latex to resolve all
% references ACM needs 'a single self-contained file'!

% TODO: Revisit this when finished
% http://www.acm.org/sigs/publications/sigguide-v2.2sp (sec 2.8)
% \balancecolumns

\end{document}
